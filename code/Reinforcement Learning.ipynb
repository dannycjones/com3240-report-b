{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive Intelligence COM3240\n",
    "\n",
    "## Lab 8: Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture overview\n",
    "\n",
    "### Future rewards\n",
    "\n",
    "It is possible for the agent to be rewarded not only when they reached the goal (future reward) but also by performing correct actions towards the goal (immediate reward).\n",
    "\n",
    "There should be a difference between the $Q(s,a)$ and $Q(s',a')$, since the future reward cannot be as good as the present reward, hence we use a discount factor, $\\gamma$.\n",
    "\n",
    "\\begin{align}\n",
    "\\Delta Q (s,a) &= \\eta\\left[r- \\left(Q(s,a) - \\gamma Q(s', a') \\right)\\right] \n",
    "\\end{align}\n",
    "\n",
    "<img src=\"http://bitsandchips.me/COM3240_Adaptive_Intelligence/Lecture8/icons/sarsa_algorithm.png\" width=\"500\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function that returns the rewards for each trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from enum import Enum, unique, auto\n",
    "from sklearn.preprocessing import normalize\n",
    "import math\n",
    "\n",
    "font_size = 13\n",
    "\n",
    "def log_progress(sequence, every=None, size=None, name=''):\n",
    "    from ipywidgets import IntProgress, HTML, HBox\n",
    "    from IPython.display import display\n",
    "\n",
    "    is_iterator = False\n",
    "    if size is None:\n",
    "        try:\n",
    "            size = len(sequence)\n",
    "        except TypeError:\n",
    "            is_iterator = True\n",
    "    if size is not None:\n",
    "        if every is None:\n",
    "            if size <= 200:\n",
    "                every = 1\n",
    "            else:\n",
    "                every = int(size / 200)     # every 0.5%\n",
    "    else:\n",
    "        assert every is not None, 'sequence is iterator, set every'\n",
    "\n",
    "    if is_iterator:\n",
    "        progress = IntProgress(min=0, max=1, value=1)\n",
    "        progress.bar_style = 'info'\n",
    "    else:\n",
    "        progress = IntProgress(min=0, max=size, value=0)\n",
    "    label = HTML()\n",
    "    box = HBox(children=[label, progress])\n",
    "    display(box)\n",
    "\n",
    "    index = 0\n",
    "    try:\n",
    "        for index, record in enumerate(sequence, 0):\n",
    "            if index % every == 0:\n",
    "                if is_iterator:\n",
    "                    label.value = '{name} {index} / ?'.format(\n",
    "                        name=name,\n",
    "                        index=index\n",
    "                    )\n",
    "                else:\n",
    "                    progress.value = index\n",
    "                    label.value = u'{name} {index} / {size}'.format(\n",
    "                        name=name,\n",
    "                        index=index,\n",
    "                        size=size\n",
    "                    )\n",
    "            yield record\n",
    "    except:\n",
    "        progress.bar_style = 'danger'\n",
    "        raise\n",
    "    else:\n",
    "        progress.bar_style = 'success'\n",
    "        index += 1\n",
    "        progress.value = index\n",
    "        label.value = \"{name} {index} / {size}\".format(\n",
    "            name=name,\n",
    "            index=str(index or '?'),\n",
    "            size=size\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# State Machine\n",
    "\n",
    "class Grid():\n",
    "    def __init__(self, shape, terminal_positions = [], rewards=[]):\n",
    "        self.grid = np.zeros(shape)\n",
    "        self.terminal_positions = terminal_positions\n",
    "        self.rewards = rewards\n",
    "        \n",
    "    def random_position(self):\n",
    "        return (\n",
    "            np.random.randint(self.get_shape()[0]),\n",
    "            np.random.randint(self.get_shape()[1])\n",
    "        )\n",
    "    \n",
    "    def get_shape(self):\n",
    "        return self.grid.shape\n",
    "    \n",
    "    def get_size(self):\n",
    "        max_x, max_y = self.get_shape()\n",
    "        return max_x * max_y\n",
    "    \n",
    "    def is_terminal(self, position):\n",
    "        return position in self.terminal_positions\n",
    "    \n",
    "    def as_vector(self, position):\n",
    "        x, y = position\n",
    "        v = np.zeros(self.get_shape())\n",
    "        v[x,y] = 1\n",
    "        return v.reshape(self.get_size(), 1)\n",
    "    \n",
    "    def get_possible_actions(self, position):\n",
    "        possible_actions = []\n",
    "        x, y = position\n",
    "        \n",
    "        if y < self.get_shape()[1] - 1:\n",
    "            possible_actions.append(Action.NORTH)\n",
    "        if x < self.get_shape()[0] - 1:\n",
    "            possible_actions.append(Action.EAST)\n",
    "        if y > 0:\n",
    "            possible_actions.append(Action.SOUTH)\n",
    "        if x > 0:\n",
    "            possible_actions.append(Action.WEST)\n",
    "            \n",
    "        return possible_actions\n",
    "    \n",
    "    def __new_position_after_action(self, position, action):\n",
    "        x, y = position\n",
    "        if action in self.get_possible_actions(position):\n",
    "            if action is Action.NORTH:\n",
    "                y += 1\n",
    "            if action is Action.EAST:\n",
    "                x += 1\n",
    "            if action is Action.SOUTH:\n",
    "                y -= 1\n",
    "            if action is Action.WEST:\n",
    "                x -= 1\n",
    "        return (x, y)\n",
    "    \n",
    "    def __reward_after_action(self, position, action):\n",
    "        x, y = position\n",
    "        if y >= self.get_shape()[1] and action is Action.NORTH:\n",
    "            return -1\n",
    "        if x >= self.get_shape()[0] and action is Action.EAST:\n",
    "            return -1\n",
    "        if y <= 0 and action is Action.SOUTH:\n",
    "            return -1\n",
    "        if x <= 0 and action is Action.WEST:\n",
    "            return -1\n",
    "        \n",
    "        for reward_segment in self.rewards:\n",
    "            loc, reward = reward_segment\n",
    "            if self.__new_position_after_action(position, action) == loc:\n",
    "                return reward\n",
    "        \n",
    "        return 0\n",
    "    \n",
    "    def perform(self, position, action):\n",
    "        return (\n",
    "            self.__new_position_after_action(position, action),\n",
    "            self.__reward_after_action(position, action)\n",
    "        )\n",
    "\n",
    "class SarsaEnum(Enum):\n",
    "    def _generate_next_value_(name, start, count, last_values):\n",
    "        return count\n",
    "    \n",
    "@unique\n",
    "class Action(SarsaEnum):\n",
    "    NORTH = auto()\n",
    "    EAST = auto()\n",
    "    SOUTH = auto()\n",
    "    WEST = auto()\n",
    "    \n",
    "    @classmethod\n",
    "    def rand(cls):\n",
    "        return cls(np.random.randint(len(cls)))\n",
    "    \n",
    "    def as_vector(self):\n",
    "        v = np.zeros((len(type(self)), 1))\n",
    "        v[self.value] = 1\n",
    "        return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_shape = (4,3)\n",
    "charging_point = (3,0)\n",
    "\n",
    "def sarsa_homing_nn(n_episodes, learning_rate, epsilon, gamma):\n",
    "    \n",
    "    grid = Grid(grid_shape, terminal_positions = [charging_point], rewards=[(charging_point, 1)])\n",
    "    max_choices = math.ceil(grid.get_size() ** 1.5)\n",
    "\n",
    "    # Initialise weights randomly. Connects input neurons (state) to output neurons (actions). Weight is Q values.\n",
    "    weights = np.random.rand(len(Action), grid.get_size())\n",
    "\n",
    "    # Define info vectors (one position for each trial) and initialise it to zero\n",
    "    r_history = np.zeros((1, n_episodes))\n",
    "    steps_req_history = np.zeros((1, n_episodes))\n",
    "\n",
    "    # Start the episode\n",
    "    for episode in range(n_episodes):\n",
    "        state = grid.random_position() # Initial state\n",
    "\n",
    "        # Define and initialise at every episode the variables needed for SARSA\n",
    "        input_v = np.zeros((grid.get_size(),1))\n",
    "        output_v = np.zeros((len(Action),1))\n",
    "        q_value = 0\n",
    "        reward = 0\n",
    "        steps_required = 0\n",
    "\n",
    "        # Simulate the steps in the episode\n",
    "        for step in range(max_choices):\n",
    "            steps_required += 1\n",
    "            input_v_ = grid.as_vector(state)\n",
    "            \n",
    "            # Compute state_q_values.  q_value = logsig(weights*input). q_value is 2x1, one value for each output neuron\n",
    "            state_q_values = (1 / (1 + np.exp(-1 * weights.dot(input_v_)))).flatten()\n",
    "            \n",
    "            winning_actions = np.argwhere(state_q_values == np.max(state_q_values)).flatten()\n",
    "\n",
    "            # Implement the policy\n",
    "            explore = np.random.rand() < epsilon\n",
    "            if explore:\n",
    "                possible_actions = grid.get_possible_actions(state)\n",
    "                action = np.random.choice(possible_actions)\n",
    "            else:\n",
    "                random_winner = np.random.choice(winning_actions)\n",
    "                action = Action(random_winner)\n",
    "\n",
    "            state, reward_ = grid.perform(state, action)\n",
    "            output_v_ = action.as_vector() # Update weights only for action selected.\n",
    "            \n",
    "            \n",
    "            # Update weights\n",
    "            q_value_ = state_q_values[action.value]\n",
    "            d_w = learning_rate * (reward - (q_value - (gamma * q_value_))) * output_v.dot(input_v.T)\n",
    "            weights += d_w\n",
    "\n",
    "            # Update variables for sarsa\n",
    "            q_value = q_value_\n",
    "            output_v = output_v_\n",
    "            input_v = input_v_\n",
    "            reward = reward_\n",
    "            \n",
    "            if grid.is_terminal(state):\n",
    "                break\n",
    "            \n",
    "\n",
    "        # Update weights for the terminal state\n",
    "        d_w = learning_rate * (reward - q_value) * output_v.dot(input_v.T)\n",
    "        weights += d_w\n",
    "\n",
    "        # Store info for the current episode\n",
    "        r_history[0,episode] = reward\n",
    "        steps_req_history[0,episode] = steps_required\n",
    "\n",
    "    return (r_history, steps_req_history, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Script that calls the function sarsa_monkey_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54b170692a894ec78f185bafb5014b39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfeb91347b36417bbcbc64e99629e2cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e66aaf7dae54e2f8e440192ba70c8f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63cc0b4eeb214c45b90429062bb1340c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bcb2529f76645d081a5836ef09c61bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b2e98ca3c1541f4af3bb74437c99c30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98bca94e40bc486e8d4f809f89fdab2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e96aa83ebbb347c9af39ca467ef1b28f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9346e056ed1413d9861f11f375baeb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0866853bd3ba4025afc61b0f855e7769",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Parameter setup\n",
    "n_episodes = 25    # int > 0\n",
    "learning_rates = [0.9, 0.1, 0.5] # real > 0\n",
    "epsilons = [0.1, 0.5, 0.9]    # real >= 0; epsilon=0 Greedy, otherwise epsilon-Greedy\n",
    "gammas = [0.9, 0.009, 0.09]         # 0 <= real <= 1\n",
    "repetitions = 250   # int > 0\n",
    "\n",
    "max_x, max_y = grid_shape\n",
    "\n",
    "weight_shape = (len(Action), max_x*max_y) \n",
    "\n",
    "# rewards = np.zeros((repetitions, n_episodes))  # reward matrix. each row contains rewards obtained in one episode\n",
    "# steps_until_terminate = np.zeros((repetitions, n_episodes))  # reward matrix. each row contains rewards obtained in one episode\n",
    "# weight_m = np.zeros((repetitions, *weight_shape))\n",
    "\n",
    "# # Start iterations over episodes\n",
    "# for j in log_progress(range(repetitions), name=\"Repetitions\"):\n",
    "#     rewards[j,:], steps_until_terminate[j,:], weight_m[j,:] = sarsa_homing_nn(n_episodes, learning_rates[0], epsilons[0], gammas[0])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lr_rewards = []\n",
    "lr_steps = []\n",
    "lr_weights = []\n",
    "\n",
    "for lr in log_progress(learning_rates, name='Learning Rates'):\n",
    "    rewards = np.zeros((repetitions, n_episodes))  # reward matrix. each row contains rewards obtained in one episode\n",
    "    steps_until_terminate = np.zeros((repetitions, n_episodes))  # reward matrix. each row contains rewards obtained in one episode\n",
    "    weight_m = np.zeros((repetitions, *weight_shape))\n",
    "\n",
    "    # Start iterations over episodes\n",
    "    for j in log_progress(range(repetitions), name=\"Repetitions\"):\n",
    "        rewards[j,:], steps_until_terminate[j,:], weight_m[j,:] = sarsa_homing_nn(n_episodes, lr, epsilons[0], gammas[0])\n",
    "    \n",
    "    lr_rewards.append(rewards)\n",
    "    lr_steps.append(steps_until_terminate)\n",
    "    lr_weights.append(weight_m)\n",
    "    \n",
    "ep_rewards = []\n",
    "ep_steps = []\n",
    "ep_weights = []\n",
    "\n",
    "for ep in log_progress(epsilons, name='Epsilons'):\n",
    "    rewards = np.zeros((repetitions, n_episodes))  # reward matrix. each row contains rewards obtained in one episode\n",
    "    steps_until_terminate = np.zeros((repetitions, n_episodes))  # reward matrix. each row contains rewards obtained in one episode\n",
    "    weight_m = np.zeros((repetitions, *weight_shape))\n",
    "\n",
    "    # Start iterations over episodes\n",
    "    for j in log_progress(range(repetitions), name=\"Repetitions\"):\n",
    "        rewards[j,:], steps_until_terminate[j,:], weight_m[j,:] = sarsa_homing_nn(n_episodes, learning_rates[0], ep, gammas[0])\n",
    "    \n",
    "    ep_rewards.append(rewards)\n",
    "    ep_steps.append(steps_until_terminate)\n",
    "    ep_weights.append(weight_m)\n",
    "    \n",
    "ga_rewards = []\n",
    "ga_steps = []\n",
    "ga_weights = []\n",
    "\n",
    "for ga in log_progress(gammas, name='Discount Rates'):\n",
    "    rewards = np.zeros((repetitions, n_episodes))  # reward matrix. each row contains rewards obtained in one episode\n",
    "    steps_until_terminate = np.zeros((repetitions, n_episodes))  # reward matrix. each row contains rewards obtained in one episode\n",
    "    weight_m = np.zeros((repetitions, *weight_shape))\n",
    "\n",
    "    # Start iterations over episodes\n",
    "    for j in log_progress(range(repetitions), name=\"Repetitions\"):\n",
    "        rewards[j,:], steps_until_terminate[j,:], weight_m[j,:] = sarsa_homing_nn(n_episodes, learning_rates[0], epsilons[0], ga)\n",
    "    \n",
    "    ga_rewards.append(rewards)\n",
    "    ga_steps.append(steps_until_terminate)\n",
    "    ga_weights.append(weight_m)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ((ax1)) = plt.subplots(1, 1, figsize=(7,4))\n",
    "\n",
    "rewards = lr_rewards[0]\n",
    "steps_until_terminate = lr_steps[0]\n",
    "\n",
    "ax1_means = np.mean(rewards, axis = 0)\n",
    "ax1_errors = 2 * np.std(rewards, axis = 0) / np.sqrt(repetitions) # errorbars are equal to twice standard error i.e. std/sqrt(samples)\n",
    "\n",
    "ax1.errorbar(np.arange(n_episodes), ax1_means, ax1_errors, 0, elinewidth = 2, capsize = 4, alpha =0.8)\n",
    "ax1.set_xlabel('Episode',fontsize = font_size)\n",
    "ax1.set_ylabel('Average Reward',fontsize = font_size)\n",
    "ax1.axis((-(n_episodes/10.0),n_episodes,-0.1,1.1))\n",
    "ax1.tick_params(axis = 'both', which='major', labelsize = 14)\n",
    "\n",
    "plt.show()  \n",
    "\n",
    "fig, ((ax2)) = plt.subplots(1, 1, figsize=(7,4))\n",
    "\n",
    "ax2_means = np.mean(steps_until_terminate, axis = 0)\n",
    "ax2_errors = 2 * np.std(steps_until_terminate, axis = 0) / np.sqrt(repetitions) # errorbars are equal to twice standard error i.e. std/sqrt(samples)\n",
    "\n",
    "ax2.errorbar(np.arange(n_episodes), ax2_means, ax2_errors, 0, elinewidth = 2, capsize = 4, alpha =0.8)\n",
    "ax2.set_xlabel('Episode',fontsize = font_size)\n",
    "ax2.set_ylabel('Average Steps',fontsize = font_size)\n",
    "ax2.set_ylim(ymin=0)\n",
    "# ax2.axis((-(n_episodes/10.0),n_episodes,-0.1,1.1))\n",
    "ax2.tick_params(axis = 'both', which='major', labelsize = 14)\n",
    "\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ((ax1)) = plt.subplots(1, 1, figsize=(6,4))\n",
    "\n",
    "weight_m = lr_weights[0]\n",
    "\n",
    "base_dir_vectors = np.array([[0,1],[1,0],[0,-1],[-1,0]])\n",
    "max_x, max_y = grid_shape\n",
    "a = np.einsum('iz,ij->zj', weight_m.mean(axis=0), base_dir_vectors)\n",
    "# a = normalize(a, axis=1, norm='l2')\n",
    "a = a.reshape((max_x, max_y, 2))\n",
    "\n",
    "x,y = np.meshgrid(np.arange(max_x),np.arange(max_y))\n",
    "charge_x, charge_y = charging_point\n",
    "\n",
    "ax1.set_title(\"Weights plotted onto the grid\")\n",
    "ax1.grid(color='#eeeeee', alpha=0.2, linestyle='-', linewidth=2, drawstyle=\"steps-post\")\n",
    "ax1.set_xticks(np.arange(max_x))\n",
    "ax1.set_yticks(np.arange(max_y))\n",
    "Q = ax1.quiver(x,y, a[:,:,0], a[:,:,1])\n",
    "ax1.scatter([charge_x],[charge_y], color='g', s=200, marker='X')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, bunchOfAxes = plt.subplots(3, 1, figsize=(8,12))\n",
    "\n",
    "axes = bunchOfAxes.flatten()\n",
    "for axes_idx, variable in enumerate([learning_rates, epsilons, gammas]):\n",
    "    ax = axes[axes_idx]\n",
    "    for idx in range(len(variable)):\n",
    "        if variable == learning_rates:\n",
    "            title = \"Varying the Learning Rate\"\n",
    "            label = \"η = \" + str(variable[idx])\n",
    "            steps = lr_steps[idx]\n",
    "        if variable == epsilons:\n",
    "            title = \"Varying Epsilon\"\n",
    "            label = \"ε = \" + str(variable[idx])\n",
    "            steps = ep_steps[idx]\n",
    "        if variable == gammas:\n",
    "            title = \"Varying Gamma\"\n",
    "            label = \"γ = \" + str(variable[idx])\n",
    "            steps = ga_steps[idx]\n",
    "            \n",
    "        ax_means = np.mean(steps, axis = 0)\n",
    "        ax_errors = 2 * np.std(steps, axis = 0) / np.sqrt(repetitions) # errorbars are equal to twice standard error i.e. std/sqrt(samples)\n",
    "\n",
    "        ax.plot(ax_means, label=label)\n",
    "        ax.set_xlabel('Episode',fontsize = font_size)\n",
    "        ax.set_ylabel('Average Steps',fontsize = font_size)\n",
    "        ax.set_ylim(ymin=0, ymax=50)\n",
    "        ax.legend()\n",
    "        ax.set_title(title)\n",
    "        ax.tick_params(axis = 'both', which='major', labelsize = 14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
